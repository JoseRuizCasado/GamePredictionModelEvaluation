{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Game Prediction Models Evaluation\n",
    "## Needed imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'plot_confusion_matrix' from 'sklearn.metrics' (C:\\Users\\ala_j\\.conda\\envs\\data-science\\lib\\site-packages\\sklearn\\metrics\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-343cab8e0405>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel_selection\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mKFold\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 12\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmetrics\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mplot_confusion_matrix\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     13\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'plot_confusion_matrix' from 'sklearn.metrics' (C:\\Users\\ala_j\\.conda\\envs\\data-science\\lib\\site-packages\\sklearn\\metrics\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "     Local_TmOffRtg  Local_TmFloor%  Local_TmDefRtg  Local_Pace  Local_TS%  \\\n0        106.326895        0.480894      100.864813   93.274918   0.579954   \n1        106.326895        0.480894      100.864813   93.274918   0.579954   \n2        106.326895        0.480894      100.864813   93.274918   0.579954   \n3        106.326895        0.480894      100.864813   93.274918   0.579954   \n4        106.326895        0.480894      100.864813   93.274918   0.579954   \n..              ...             ...             ...         ...        ...   \n627      106.494585        0.480980      119.635230   87.074037   0.566133   \n628      106.494585        0.480980      119.635230   87.074037   0.566133   \n629      106.494585        0.480980      119.635230   87.074037   0.566133   \n630      106.494585        0.480980      119.635230   87.074037   0.566133   \n631      106.494585        0.480980      119.635230   87.074037   0.566133   \n\n     Local_eFG%  Local_FTARate  Local_3FGARate  Local_TmOR%  Local_TmDR%  ...  \\\n0      0.514913       0.241137        0.316263    20.654628    74.140753  ...   \n1      0.514913       0.241137        0.316263    20.654628    74.140753  ...   \n2      0.514913       0.241137        0.316263    20.654628    74.140753  ...   \n3      0.514913       0.241137        0.316263    20.654628    74.140753  ...   \n4      0.514913       0.241137        0.316263    20.654628    74.140753  ...   \n..          ...            ...             ...          ...          ...  ...   \n627    0.487970       0.290705        0.318392    19.164345    77.393453  ...   \n628    0.487970       0.290705        0.318392    19.164345    77.393453  ...   \n629    0.487970       0.290705        0.318392    19.164345    77.393453  ...   \n630    0.487970       0.290705        0.318392    19.164345    77.393453  ...   \n631    0.487970       0.290705        0.318392    19.164345    77.393453  ...   \n\n     Visitor_TS%  Visitor_eFG%  Visitor_FTARate  Visitor_3FGARate  \\\n0       0.538690      0.477319         0.296446          0.266108   \n1       0.566133      0.487970         0.290705          0.318392   \n2       0.535670      0.477748         0.217507          0.206307   \n3       0.571810      0.508089         0.253146          0.282205   \n4       0.567381      0.478923         0.323839          0.192538   \n..           ...           ...              ...               ...   \n627     0.593453      0.523714         0.321761          0.302852   \n628     0.601352      0.521150         0.309096          0.252419   \n629     0.579954      0.514913         0.241137          0.316263   \n630     0.564315      0.490814         0.288426          0.299755   \n631     0.557686      0.482662         0.261403          0.255802   \n\n     Visitor_TmOR%  Visitor_TmDR%  Visitor_BLK%  Visitor_TOV%  Visitor_STL%  \\\n0        28.615702      77.177177      5.442698     12.049168      7.804251   \n1        19.164345      77.393453      8.911307     10.418630      6.645854   \n2        23.866896      76.325088      6.382061     13.575965      7.993861   \n3        21.128451      76.812500      6.345733     13.156047      9.519718   \n4        24.899598      76.502732      8.295123     13.517942      8.438790   \n..             ...            ...           ...           ...           ...   \n627      22.086638      74.622532      9.150613     11.785222      8.670923   \n628      31.235828      75.572519     10.282776     13.567750      7.846969   \n629      20.654628      74.140753      9.227723     13.432928     10.002664   \n630      25.589837      75.406758      7.170966     13.369487      8.775888   \n631      25.108696      75.139807      9.274194     12.258092      6.531878   \n\n     Win  \n0      0  \n1      1  \n2      1  \n3      1  \n4      0  \n..   ...  \n627    0  \n628    0  \n629    1  \n630    1  \n631    1  \n\n[632 rows x 27 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Local_TmOffRtg</th>\n      <th>Local_TmFloor%</th>\n      <th>Local_TmDefRtg</th>\n      <th>Local_Pace</th>\n      <th>Local_TS%</th>\n      <th>Local_eFG%</th>\n      <th>Local_FTARate</th>\n      <th>Local_3FGARate</th>\n      <th>Local_TmOR%</th>\n      <th>Local_TmDR%</th>\n      <th>...</th>\n      <th>Visitor_TS%</th>\n      <th>Visitor_eFG%</th>\n      <th>Visitor_FTARate</th>\n      <th>Visitor_3FGARate</th>\n      <th>Visitor_TmOR%</th>\n      <th>Visitor_TmDR%</th>\n      <th>Visitor_BLK%</th>\n      <th>Visitor_TOV%</th>\n      <th>Visitor_STL%</th>\n      <th>Win</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>106.326895</td>\n      <td>0.480894</td>\n      <td>100.864813</td>\n      <td>93.274918</td>\n      <td>0.579954</td>\n      <td>0.514913</td>\n      <td>0.241137</td>\n      <td>0.316263</td>\n      <td>20.654628</td>\n      <td>74.140753</td>\n      <td>...</td>\n      <td>0.538690</td>\n      <td>0.477319</td>\n      <td>0.296446</td>\n      <td>0.266108</td>\n      <td>28.615702</td>\n      <td>77.177177</td>\n      <td>5.442698</td>\n      <td>12.049168</td>\n      <td>7.804251</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>106.326895</td>\n      <td>0.480894</td>\n      <td>100.864813</td>\n      <td>93.274918</td>\n      <td>0.579954</td>\n      <td>0.514913</td>\n      <td>0.241137</td>\n      <td>0.316263</td>\n      <td>20.654628</td>\n      <td>74.140753</td>\n      <td>...</td>\n      <td>0.566133</td>\n      <td>0.487970</td>\n      <td>0.290705</td>\n      <td>0.318392</td>\n      <td>19.164345</td>\n      <td>77.393453</td>\n      <td>8.911307</td>\n      <td>10.418630</td>\n      <td>6.645854</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>106.326895</td>\n      <td>0.480894</td>\n      <td>100.864813</td>\n      <td>93.274918</td>\n      <td>0.579954</td>\n      <td>0.514913</td>\n      <td>0.241137</td>\n      <td>0.316263</td>\n      <td>20.654628</td>\n      <td>74.140753</td>\n      <td>...</td>\n      <td>0.535670</td>\n      <td>0.477748</td>\n      <td>0.217507</td>\n      <td>0.206307</td>\n      <td>23.866896</td>\n      <td>76.325088</td>\n      <td>6.382061</td>\n      <td>13.575965</td>\n      <td>7.993861</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>106.326895</td>\n      <td>0.480894</td>\n      <td>100.864813</td>\n      <td>93.274918</td>\n      <td>0.579954</td>\n      <td>0.514913</td>\n      <td>0.241137</td>\n      <td>0.316263</td>\n      <td>20.654628</td>\n      <td>74.140753</td>\n      <td>...</td>\n      <td>0.571810</td>\n      <td>0.508089</td>\n      <td>0.253146</td>\n      <td>0.282205</td>\n      <td>21.128451</td>\n      <td>76.812500</td>\n      <td>6.345733</td>\n      <td>13.156047</td>\n      <td>9.519718</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>106.326895</td>\n      <td>0.480894</td>\n      <td>100.864813</td>\n      <td>93.274918</td>\n      <td>0.579954</td>\n      <td>0.514913</td>\n      <td>0.241137</td>\n      <td>0.316263</td>\n      <td>20.654628</td>\n      <td>74.140753</td>\n      <td>...</td>\n      <td>0.567381</td>\n      <td>0.478923</td>\n      <td>0.323839</td>\n      <td>0.192538</td>\n      <td>24.899598</td>\n      <td>76.502732</td>\n      <td>8.295123</td>\n      <td>13.517942</td>\n      <td>8.438790</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>627</th>\n      <td>106.494585</td>\n      <td>0.480980</td>\n      <td>119.635230</td>\n      <td>87.074037</td>\n      <td>0.566133</td>\n      <td>0.487970</td>\n      <td>0.290705</td>\n      <td>0.318392</td>\n      <td>19.164345</td>\n      <td>77.393453</td>\n      <td>...</td>\n      <td>0.593453</td>\n      <td>0.523714</td>\n      <td>0.321761</td>\n      <td>0.302852</td>\n      <td>22.086638</td>\n      <td>74.622532</td>\n      <td>9.150613</td>\n      <td>11.785222</td>\n      <td>8.670923</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>628</th>\n      <td>106.494585</td>\n      <td>0.480980</td>\n      <td>119.635230</td>\n      <td>87.074037</td>\n      <td>0.566133</td>\n      <td>0.487970</td>\n      <td>0.290705</td>\n      <td>0.318392</td>\n      <td>19.164345</td>\n      <td>77.393453</td>\n      <td>...</td>\n      <td>0.601352</td>\n      <td>0.521150</td>\n      <td>0.309096</td>\n      <td>0.252419</td>\n      <td>31.235828</td>\n      <td>75.572519</td>\n      <td>10.282776</td>\n      <td>13.567750</td>\n      <td>7.846969</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>629</th>\n      <td>106.494585</td>\n      <td>0.480980</td>\n      <td>119.635230</td>\n      <td>87.074037</td>\n      <td>0.566133</td>\n      <td>0.487970</td>\n      <td>0.290705</td>\n      <td>0.318392</td>\n      <td>19.164345</td>\n      <td>77.393453</td>\n      <td>...</td>\n      <td>0.579954</td>\n      <td>0.514913</td>\n      <td>0.241137</td>\n      <td>0.316263</td>\n      <td>20.654628</td>\n      <td>74.140753</td>\n      <td>9.227723</td>\n      <td>13.432928</td>\n      <td>10.002664</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>630</th>\n      <td>106.494585</td>\n      <td>0.480980</td>\n      <td>119.635230</td>\n      <td>87.074037</td>\n      <td>0.566133</td>\n      <td>0.487970</td>\n      <td>0.290705</td>\n      <td>0.318392</td>\n      <td>19.164345</td>\n      <td>77.393453</td>\n      <td>...</td>\n      <td>0.564315</td>\n      <td>0.490814</td>\n      <td>0.288426</td>\n      <td>0.299755</td>\n      <td>25.589837</td>\n      <td>75.406758</td>\n      <td>7.170966</td>\n      <td>13.369487</td>\n      <td>8.775888</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>631</th>\n      <td>106.494585</td>\n      <td>0.480980</td>\n      <td>119.635230</td>\n      <td>87.074037</td>\n      <td>0.566133</td>\n      <td>0.487970</td>\n      <td>0.290705</td>\n      <td>0.318392</td>\n      <td>19.164345</td>\n      <td>77.393453</td>\n      <td>...</td>\n      <td>0.557686</td>\n      <td>0.482662</td>\n      <td>0.261403</td>\n      <td>0.255802</td>\n      <td>25.108696</td>\n      <td>75.139807</td>\n      <td>9.274194</td>\n      <td>12.258092</td>\n      <td>6.531878</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>632 rows × 27 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('PredictGamesDataset.csv')\n",
    "del data['Local_Team_id']\n",
    "del data['Visitor_Team_id']\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we need to divide the data set into input and class for each dataset row."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "     Local_TmOffRtg  Local_TmFloor%  Local_TmDefRtg  Local_Pace  Local_TS%  \\\n0        106.326895        0.480894      100.864813   93.274918   0.579954   \n1        106.326895        0.480894      100.864813   93.274918   0.579954   \n2        106.326895        0.480894      100.864813   93.274918   0.579954   \n3        106.326895        0.480894      100.864813   93.274918   0.579954   \n4        106.326895        0.480894      100.864813   93.274918   0.579954   \n..              ...             ...             ...         ...        ...   \n627      106.494585        0.480980      119.635230   87.074037   0.566133   \n628      106.494585        0.480980      119.635230   87.074037   0.566133   \n629      106.494585        0.480980      119.635230   87.074037   0.566133   \n630      106.494585        0.480980      119.635230   87.074037   0.566133   \n631      106.494585        0.480980      119.635230   87.074037   0.566133   \n\n     Local_eFG%  Local_FTARate  Local_3FGARate  Local_TmOR%  Local_TmDR%  ...  \\\n0      0.514913       0.241137        0.316263    20.654628    74.140753  ...   \n1      0.514913       0.241137        0.316263    20.654628    74.140753  ...   \n2      0.514913       0.241137        0.316263    20.654628    74.140753  ...   \n3      0.514913       0.241137        0.316263    20.654628    74.140753  ...   \n4      0.514913       0.241137        0.316263    20.654628    74.140753  ...   \n..          ...            ...             ...          ...          ...  ...   \n627    0.487970       0.290705        0.318392    19.164345    77.393453  ...   \n628    0.487970       0.290705        0.318392    19.164345    77.393453  ...   \n629    0.487970       0.290705        0.318392    19.164345    77.393453  ...   \n630    0.487970       0.290705        0.318392    19.164345    77.393453  ...   \n631    0.487970       0.290705        0.318392    19.164345    77.393453  ...   \n\n     Visitor_Pace  Visitor_TS%  Visitor_eFG%  Visitor_FTARate  \\\n0       91.701077     0.538690      0.477319         0.296446   \n1       87.074037     0.566133      0.487970         0.290705   \n2       91.688075     0.535670      0.477748         0.217507   \n3       94.194271     0.571810      0.508089         0.253146   \n4       90.892768     0.567381      0.478923         0.323839   \n..            ...          ...           ...              ...   \n627     93.469083     0.593453      0.523714         0.321761   \n628     92.665032     0.601352      0.521150         0.309096   \n629     93.274918     0.579954      0.514913         0.241137   \n630     87.274997     0.564315      0.490814         0.288426   \n631     90.798947     0.557686      0.482662         0.261403   \n\n     Visitor_3FGARate  Visitor_TmOR%  Visitor_TmDR%  Visitor_BLK%  \\\n0            0.266108      28.615702      77.177177      5.442698   \n1            0.318392      19.164345      77.393453      8.911307   \n2            0.206307      23.866896      76.325088      6.382061   \n3            0.282205      21.128451      76.812500      6.345733   \n4            0.192538      24.899598      76.502732      8.295123   \n..                ...            ...            ...           ...   \n627          0.302852      22.086638      74.622532      9.150613   \n628          0.252419      31.235828      75.572519     10.282776   \n629          0.316263      20.654628      74.140753      9.227723   \n630          0.299755      25.589837      75.406758      7.170966   \n631          0.255802      25.108696      75.139807      9.274194   \n\n     Visitor_TOV%  Visitor_STL%  \n0       12.049168      7.804251  \n1       10.418630      6.645854  \n2       13.575965      7.993861  \n3       13.156047      9.519718  \n4       13.517942      8.438790  \n..            ...           ...  \n627     11.785222      8.670923  \n628     13.567750      7.846969  \n629     13.432928     10.002664  \n630     13.369487      8.775888  \n631     12.258092      6.531878  \n\n[632 rows x 26 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Local_TmOffRtg</th>\n      <th>Local_TmFloor%</th>\n      <th>Local_TmDefRtg</th>\n      <th>Local_Pace</th>\n      <th>Local_TS%</th>\n      <th>Local_eFG%</th>\n      <th>Local_FTARate</th>\n      <th>Local_3FGARate</th>\n      <th>Local_TmOR%</th>\n      <th>Local_TmDR%</th>\n      <th>...</th>\n      <th>Visitor_Pace</th>\n      <th>Visitor_TS%</th>\n      <th>Visitor_eFG%</th>\n      <th>Visitor_FTARate</th>\n      <th>Visitor_3FGARate</th>\n      <th>Visitor_TmOR%</th>\n      <th>Visitor_TmDR%</th>\n      <th>Visitor_BLK%</th>\n      <th>Visitor_TOV%</th>\n      <th>Visitor_STL%</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>106.326895</td>\n      <td>0.480894</td>\n      <td>100.864813</td>\n      <td>93.274918</td>\n      <td>0.579954</td>\n      <td>0.514913</td>\n      <td>0.241137</td>\n      <td>0.316263</td>\n      <td>20.654628</td>\n      <td>74.140753</td>\n      <td>...</td>\n      <td>91.701077</td>\n      <td>0.538690</td>\n      <td>0.477319</td>\n      <td>0.296446</td>\n      <td>0.266108</td>\n      <td>28.615702</td>\n      <td>77.177177</td>\n      <td>5.442698</td>\n      <td>12.049168</td>\n      <td>7.804251</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>106.326895</td>\n      <td>0.480894</td>\n      <td>100.864813</td>\n      <td>93.274918</td>\n      <td>0.579954</td>\n      <td>0.514913</td>\n      <td>0.241137</td>\n      <td>0.316263</td>\n      <td>20.654628</td>\n      <td>74.140753</td>\n      <td>...</td>\n      <td>87.074037</td>\n      <td>0.566133</td>\n      <td>0.487970</td>\n      <td>0.290705</td>\n      <td>0.318392</td>\n      <td>19.164345</td>\n      <td>77.393453</td>\n      <td>8.911307</td>\n      <td>10.418630</td>\n      <td>6.645854</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>106.326895</td>\n      <td>0.480894</td>\n      <td>100.864813</td>\n      <td>93.274918</td>\n      <td>0.579954</td>\n      <td>0.514913</td>\n      <td>0.241137</td>\n      <td>0.316263</td>\n      <td>20.654628</td>\n      <td>74.140753</td>\n      <td>...</td>\n      <td>91.688075</td>\n      <td>0.535670</td>\n      <td>0.477748</td>\n      <td>0.217507</td>\n      <td>0.206307</td>\n      <td>23.866896</td>\n      <td>76.325088</td>\n      <td>6.382061</td>\n      <td>13.575965</td>\n      <td>7.993861</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>106.326895</td>\n      <td>0.480894</td>\n      <td>100.864813</td>\n      <td>93.274918</td>\n      <td>0.579954</td>\n      <td>0.514913</td>\n      <td>0.241137</td>\n      <td>0.316263</td>\n      <td>20.654628</td>\n      <td>74.140753</td>\n      <td>...</td>\n      <td>94.194271</td>\n      <td>0.571810</td>\n      <td>0.508089</td>\n      <td>0.253146</td>\n      <td>0.282205</td>\n      <td>21.128451</td>\n      <td>76.812500</td>\n      <td>6.345733</td>\n      <td>13.156047</td>\n      <td>9.519718</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>106.326895</td>\n      <td>0.480894</td>\n      <td>100.864813</td>\n      <td>93.274918</td>\n      <td>0.579954</td>\n      <td>0.514913</td>\n      <td>0.241137</td>\n      <td>0.316263</td>\n      <td>20.654628</td>\n      <td>74.140753</td>\n      <td>...</td>\n      <td>90.892768</td>\n      <td>0.567381</td>\n      <td>0.478923</td>\n      <td>0.323839</td>\n      <td>0.192538</td>\n      <td>24.899598</td>\n      <td>76.502732</td>\n      <td>8.295123</td>\n      <td>13.517942</td>\n      <td>8.438790</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>627</th>\n      <td>106.494585</td>\n      <td>0.480980</td>\n      <td>119.635230</td>\n      <td>87.074037</td>\n      <td>0.566133</td>\n      <td>0.487970</td>\n      <td>0.290705</td>\n      <td>0.318392</td>\n      <td>19.164345</td>\n      <td>77.393453</td>\n      <td>...</td>\n      <td>93.469083</td>\n      <td>0.593453</td>\n      <td>0.523714</td>\n      <td>0.321761</td>\n      <td>0.302852</td>\n      <td>22.086638</td>\n      <td>74.622532</td>\n      <td>9.150613</td>\n      <td>11.785222</td>\n      <td>8.670923</td>\n    </tr>\n    <tr>\n      <th>628</th>\n      <td>106.494585</td>\n      <td>0.480980</td>\n      <td>119.635230</td>\n      <td>87.074037</td>\n      <td>0.566133</td>\n      <td>0.487970</td>\n      <td>0.290705</td>\n      <td>0.318392</td>\n      <td>19.164345</td>\n      <td>77.393453</td>\n      <td>...</td>\n      <td>92.665032</td>\n      <td>0.601352</td>\n      <td>0.521150</td>\n      <td>0.309096</td>\n      <td>0.252419</td>\n      <td>31.235828</td>\n      <td>75.572519</td>\n      <td>10.282776</td>\n      <td>13.567750</td>\n      <td>7.846969</td>\n    </tr>\n    <tr>\n      <th>629</th>\n      <td>106.494585</td>\n      <td>0.480980</td>\n      <td>119.635230</td>\n      <td>87.074037</td>\n      <td>0.566133</td>\n      <td>0.487970</td>\n      <td>0.290705</td>\n      <td>0.318392</td>\n      <td>19.164345</td>\n      <td>77.393453</td>\n      <td>...</td>\n      <td>93.274918</td>\n      <td>0.579954</td>\n      <td>0.514913</td>\n      <td>0.241137</td>\n      <td>0.316263</td>\n      <td>20.654628</td>\n      <td>74.140753</td>\n      <td>9.227723</td>\n      <td>13.432928</td>\n      <td>10.002664</td>\n    </tr>\n    <tr>\n      <th>630</th>\n      <td>106.494585</td>\n      <td>0.480980</td>\n      <td>119.635230</td>\n      <td>87.074037</td>\n      <td>0.566133</td>\n      <td>0.487970</td>\n      <td>0.290705</td>\n      <td>0.318392</td>\n      <td>19.164345</td>\n      <td>77.393453</td>\n      <td>...</td>\n      <td>87.274997</td>\n      <td>0.564315</td>\n      <td>0.490814</td>\n      <td>0.288426</td>\n      <td>0.299755</td>\n      <td>25.589837</td>\n      <td>75.406758</td>\n      <td>7.170966</td>\n      <td>13.369487</td>\n      <td>8.775888</td>\n    </tr>\n    <tr>\n      <th>631</th>\n      <td>106.494585</td>\n      <td>0.480980</td>\n      <td>119.635230</td>\n      <td>87.074037</td>\n      <td>0.566133</td>\n      <td>0.487970</td>\n      <td>0.290705</td>\n      <td>0.318392</td>\n      <td>19.164345</td>\n      <td>77.393453</td>\n      <td>...</td>\n      <td>90.798947</td>\n      <td>0.557686</td>\n      <td>0.482662</td>\n      <td>0.261403</td>\n      <td>0.255802</td>\n      <td>25.108696</td>\n      <td>75.139807</td>\n      <td>9.274194</td>\n      <td>12.258092</td>\n      <td>6.531878</td>\n    </tr>\n  </tbody>\n</table>\n<p>632 rows × 26 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_unscaled = data.iloc[:, :26]\n",
    "X_unscaled"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "0      0\n1      1\n2      1\n3      1\n4      0\n      ..\n627    0\n628    0\n629    1\n630    1\n631    1\nName: Win, Length: 632, dtype: int64"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = data.iloc[:, 26]\n",
    "Y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As seen in the input data it is necessary to pre-process the data. Differences in the scales across input variables may\n",
    "increase the difficulty of the problem being modeled. An example of this is that large input values can result in a\n",
    "model that learns large weight values. A model with large weight values is often unstable, meaning that it may suffer\n",
    "from poor performance during learning and sensitivity to input values resulting in higher generalization error."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.02104002, -0.3888855 , -0.81763465, ..., -1.80736562,\n        -0.85192723, -0.33766863],\n       [ 0.02104002, -0.3888855 , -0.81763465, ...,  0.49753131,\n        -2.40555684, -1.34760684],\n       [ 0.02104002, -0.3888855 , -0.81763465, ..., -1.18315728,\n         0.6028549 , -0.17235959],\n       ...,\n       [ 0.06623411, -0.38345585,  2.03779632, ...,  0.70778998,\n         0.46656447,  1.57899721],\n       [ 0.06623411, -0.38345585,  2.03779632, ..., -0.65892841,\n         0.40611638,  0.50944393],\n       [ 0.06623411, -0.38345585,  2.03779632, ...,  0.7386699 ,\n        -0.65285782, -1.44697559]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_unscaled)\n",
    "X = scaler.transform(X_unscaled)\n",
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Whe we the have data scaled, it is time to divide the original data set into training set and test set. The test set\n",
    "size will be th 20% of the original dataset. Stratifying the data by the results, we manage to maintain the percentages\n",
    "of each class in both resulting sets."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building Classifier Models\n",
    "We are going to evaluate different models:\n",
    "* Logistic Regression. All different solver will be evaluates (lbfgs, newton-cg, liblinear, sag, saga).\n",
    "* Support Vector Machines. For classification Linear Support Vector Classifier will be used.\n",
    "* Random Forest.\n",
    "* Neuronal Network. For this classification Multi Layer Perceptron Classifier will be evaluated with different solvers\n",
    "(lbfgs, sgd, adam)\n",
    "\n",
    "The models will trained  in loop for 30 times to obtain the mean accuracy, then we will calculate the deviation to\n",
    "display in a plot. In each iteration of the loop the dataset will be splited in training and test set."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ala_j\\.conda\\envs\\tfg-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\ala_j\\.conda\\envs\\tfg-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\ala_j\\.conda\\envs\\tfg-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\ala_j\\.conda\\envs\\tfg-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\ala_j\\.conda\\envs\\tfg-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\ala_j\\.conda\\envs\\tfg-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\ala_j\\.conda\\envs\\tfg-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\ala_j\\.conda\\envs\\tfg-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\ala_j\\.conda\\envs\\tfg-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\ala_j\\.conda\\envs\\tfg-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\ala_j\\.conda\\envs\\tfg-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\ala_j\\.conda\\envs\\tfg-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\ala_j\\.conda\\envs\\tfg-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\ala_j\\.conda\\envs\\tfg-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\ala_j\\.conda\\envs\\tfg-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\ala_j\\.conda\\envs\\tfg-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\ala_j\\.conda\\envs\\tfg-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\ala_j\\.conda\\envs\\tfg-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\ala_j\\.conda\\envs\\tfg-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\ala_j\\.conda\\envs\\tfg-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\ala_j\\.conda\\envs\\tfg-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\ala_j\\.conda\\envs\\tfg-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\ala_j\\.conda\\envs\\tfg-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\ala_j\\.conda\\envs\\tfg-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\ala_j\\.conda\\envs\\tfg-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\ala_j\\.conda\\envs\\tfg-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\ala_j\\.conda\\envs\\tfg-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\ala_j\\.conda\\envs\\tfg-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\ala_j\\.conda\\envs\\tfg-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\ala_j\\.conda\\envs\\tfg-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression solver=lbfgs -> train scores: [0.693069306930693, 0.7168316831683168, 0.7029702970297029, 0.6831683168316832, 0.6831683168316832, 0.7069306930693069, 0.6772277227722773, 0.7029702970297029, 0.689108910891089, 0.699009900990099, 0.699009900990099, 0.693069306930693, 0.7108910891089109, 0.7069306930693069, 0.700990099009901, 0.7148514851485148, 0.693069306930693, 0.7029702970297029, 0.7108910891089109, 0.700990099009901, 0.695049504950495, 0.695049504950495, 0.695049504950495, 0.699009900990099, 0.7227722772277227, 0.6831683168316832, 0.7267326732673267, 0.7168316831683168, 0.699009900990099, 0.7089108910891089] test scores: [0.7480314960629921, 0.6850393700787402, 0.6535433070866141, 0.7086614173228346, 0.7322834645669292, 0.6614173228346457, 0.7716535433070866, 0.6535433070866141, 0.6929133858267716, 0.6614173228346457, 0.7086614173228346, 0.6692913385826772, 0.6535433070866141, 0.6692913385826772, 0.6771653543307087, 0.6535433070866141, 0.7165354330708661, 0.6535433070866141, 0.6141732283464567, 0.6771653543307087, 0.7244094488188977, 0.6850393700787402, 0.7007874015748031, 0.6299212598425197, 0.5826771653543307, 0.6377952755905512, 0.6535433070866141, 0.6456692913385826, 0.6456692913385826, 0.6692913385826772]\n",
      "Logistic Regression solver=newton-cg -> train scores: [0.693069306930693, 0.7168316831683168, 0.7029702970297029, 0.6831683168316832, 0.6831683168316832, 0.7069306930693069, 0.6772277227722773, 0.7029702970297029, 0.689108910891089, 0.699009900990099, 0.699009900990099, 0.693069306930693, 0.7108910891089109, 0.7069306930693069, 0.700990099009901, 0.7148514851485148, 0.693069306930693, 0.7029702970297029, 0.7108910891089109, 0.700990099009901, 0.695049504950495, 0.695049504950495, 0.695049504950495, 0.699009900990099, 0.7227722772277227, 0.6831683168316832, 0.7267326732673267, 0.7168316831683168, 0.699009900990099, 0.7089108910891089] test scores: [0.7480314960629921, 0.6850393700787402, 0.6535433070866141, 0.7086614173228346, 0.7322834645669292, 0.6614173228346457, 0.7716535433070866, 0.6535433070866141, 0.6929133858267716, 0.6614173228346457, 0.7086614173228346, 0.6692913385826772, 0.6535433070866141, 0.6692913385826772, 0.6771653543307087, 0.6535433070866141, 0.7165354330708661, 0.6535433070866141, 0.6141732283464567, 0.6771653543307087, 0.7244094488188977, 0.6850393700787402, 0.7007874015748031, 0.6299212598425197, 0.5826771653543307, 0.6377952755905512, 0.6535433070866141, 0.6456692913385826, 0.6456692913385826, 0.6692913385826772]\n",
      "Logistic Regression solver=liblinear -> train scores: [0.695049504950495, 0.7148514851485148, 0.700990099009901, 0.6831683168316832, 0.6831683168316832, 0.7069306930693069, 0.6772277227722773, 0.7029702970297029, 0.689108910891089, 0.697029702970297, 0.699009900990099, 0.693069306930693, 0.7108910891089109, 0.7089108910891089, 0.700990099009901, 0.7148514851485148, 0.693069306930693, 0.7049504950495049, 0.7148514851485148, 0.700990099009901, 0.695049504950495, 0.693069306930693, 0.695049504950495, 0.700990099009901, 0.7227722772277227, 0.6831683168316832, 0.7267326732673267, 0.7128712871287128, 0.699009900990099, 0.7089108910891089] test scores: [0.7480314960629921, 0.6850393700787402, 0.6456692913385826, 0.7086614173228346, 0.7322834645669292, 0.6614173228346457, 0.7716535433070866, 0.6535433070866141, 0.6929133858267716, 0.6614173228346457, 0.7086614173228346, 0.6692913385826772, 0.6535433070866141, 0.6692913385826772, 0.6771653543307087, 0.6535433070866141, 0.7165354330708661, 0.6535433070866141, 0.6141732283464567, 0.6771653543307087, 0.7244094488188977, 0.6929133858267716, 0.7007874015748031, 0.6299212598425197, 0.5826771653543307, 0.6377952755905512, 0.6535433070866141, 0.6456692913385826, 0.6377952755905512, 0.6692913385826772]\n",
      "Logistic Regression solver=sag -> train scores: [0.693069306930693, 0.7168316831683168, 0.7029702970297029, 0.6831683168316832, 0.6831683168316832, 0.7069306930693069, 0.6772277227722773, 0.7029702970297029, 0.689108910891089, 0.699009900990099, 0.699009900990099, 0.693069306930693, 0.7108910891089109, 0.7069306930693069, 0.700990099009901, 0.7148514851485148, 0.693069306930693, 0.7029702970297029, 0.7108910891089109, 0.700990099009901, 0.695049504950495, 0.695049504950495, 0.695049504950495, 0.699009900990099, 0.7227722772277227, 0.6831683168316832, 0.7267326732673267, 0.7168316831683168, 0.699009900990099, 0.7089108910891089] test scores: [0.7480314960629921, 0.6850393700787402, 0.6535433070866141, 0.7086614173228346, 0.7322834645669292, 0.6614173228346457, 0.7716535433070866, 0.6535433070866141, 0.6929133858267716, 0.6614173228346457, 0.7086614173228346, 0.6692913385826772, 0.6535433070866141, 0.6692913385826772, 0.6771653543307087, 0.6535433070866141, 0.7165354330708661, 0.6535433070866141, 0.6141732283464567, 0.6771653543307087, 0.7244094488188977, 0.6850393700787402, 0.7007874015748031, 0.6299212598425197, 0.5826771653543307, 0.6377952755905512, 0.6535433070866141, 0.6456692913385826, 0.6456692913385826, 0.6692913385826772]\n",
      "Logistic Regression solver=saga -> train scores: [0.693069306930693, 0.7168316831683168, 0.7029702970297029, 0.6831683168316832, 0.6831683168316832, 0.7069306930693069, 0.6772277227722773, 0.7029702970297029, 0.689108910891089, 0.697029702970297, 0.699009900990099, 0.693069306930693, 0.7108910891089109, 0.7069306930693069, 0.700990099009901, 0.7148514851485148, 0.693069306930693, 0.7029702970297029, 0.7108910891089109, 0.700990099009901, 0.695049504950495, 0.695049504950495, 0.695049504950495, 0.699009900990099, 0.7227722772277227, 0.6851485148514852, 0.7267326732673267, 0.7168316831683168, 0.699009900990099, 0.7089108910891089] test scores: [0.7480314960629921, 0.6850393700787402, 0.6535433070866141, 0.7086614173228346, 0.7322834645669292, 0.6614173228346457, 0.7716535433070866, 0.6535433070866141, 0.6929133858267716, 0.6614173228346457, 0.7086614173228346, 0.6692913385826772, 0.6535433070866141, 0.6692913385826772, 0.6771653543307087, 0.6535433070866141, 0.7165354330708661, 0.6535433070866141, 0.6141732283464567, 0.6771653543307087, 0.7244094488188977, 0.6850393700787402, 0.7007874015748031, 0.6299212598425197, 0.5826771653543307, 0.6377952755905512, 0.6535433070866141, 0.6456692913385826, 0.6456692913385826, 0.6692913385826772]\n",
      "Support Vector Machines -> train scores: [0.700990099009901, 0.7049504950495049, 0.7069306930693069, 0.691089108910891, 0.6732673267326733, 0.7148514851485148, 0.6752475247524753, 0.7128712871287128, 0.691089108910891, 0.695049504950495, 0.700990099009901, 0.7049504950495049, 0.7049504950495049, 0.7188118811881188, 0.697029702970297, 0.7148514851485148, 0.699009900990099, 0.695049504950495, 0.7089108910891089, 0.7049504950495049, 0.700990099009901, 0.7128712871287128, 0.7049504950495049, 0.695049504950495, 0.7168316831683168, 0.7049504950495049, 0.7267326732673267, 0.7247524752475247, 0.699009900990099, 0.7128712871287128] test scores: [0.7244094488188977, 0.6614173228346457, 0.6220472440944882, 0.6929133858267716, 0.7086614173228346, 0.6299212598425197, 0.7322834645669292, 0.6456692913385826, 0.6850393700787402, 0.6614173228346457, 0.6771653543307087, 0.6456692913385826, 0.6141732283464567, 0.6692913385826772, 0.6535433070866141, 0.6220472440944882, 0.7007874015748031, 0.6220472440944882, 0.6299212598425197, 0.6929133858267716, 0.7007874015748031, 0.6850393700787402, 0.6850393700787402, 0.6535433070866141, 0.6141732283464567, 0.6377952755905512, 0.6377952755905512, 0.6456692913385826, 0.6614173228346457, 0.6692913385826772]\n",
      "Random Forest -> train scores: [0.9564356435643564, 0.9405940594059405, 0.9564356435643564, 0.9386138613861386, 0.9564356435643564, 0.9603960396039604, 0.9445544554455445, 0.9524752475247524, 0.9564356435643564, 0.9465346534653465, 0.9524752475247524, 0.9524752475247524, 0.9504950495049505, 0.9544554455445544, 0.9524752475247524, 0.9425742574257425, 0.9425742574257425, 0.9465346534653465, 0.9544554455445544, 0.9524752475247524, 0.9504950495049505, 0.9465346534653465, 0.9564356435643564, 0.9584158415841584, 0.9544554455445544, 0.9564356435643564, 0.9544554455445544, 0.9425742574257425, 0.9564356435643564, 0.9564356435643564] test scores: [0.6220472440944882, 0.6535433070866141, 0.5669291338582677, 0.6850393700787402, 0.6456692913385826, 0.6062992125984252, 0.7244094488188977, 0.6377952755905512, 0.5984251968503937, 0.6299212598425197, 0.6220472440944882, 0.6299212598425197, 0.6299212598425197, 0.6141732283464567, 0.5905511811023622, 0.5748031496062992, 0.6220472440944882, 0.6299212598425197, 0.6456692913385826, 0.6062992125984252, 0.6535433070866141, 0.6220472440944882, 0.6299212598425197, 0.5984251968503937, 0.5748031496062992, 0.5905511811023622, 0.6141732283464567, 0.5669291338582677, 0.6377952755905512, 0.5748031496062992]\n",
      "Neuronal Network solver=lbfgs -> train scores: [0.9564356435643564, 0.9405940594059405, 0.9564356435643564, 0.9386138613861386, 0.9564356435643564, 0.9603960396039604, 0.9445544554455445, 0.9524752475247524, 0.9564356435643564, 0.9465346534653465, 0.9524752475247524, 0.9524752475247524, 0.9504950495049505, 0.9544554455445544, 0.9524752475247524, 0.9425742574257425, 0.9425742574257425, 0.9465346534653465, 0.9544554455445544, 0.9524752475247524, 0.9504950495049505, 0.9465346534653465, 0.9564356435643564, 0.9584158415841584, 0.9544554455445544, 0.9564356435643564, 0.9544554455445544, 0.9425742574257425, 0.9564356435643564, 0.9564356435643564] test scores: [0.6062992125984252, 0.6141732283464567, 0.6062992125984252, 0.6220472440944882, 0.5984251968503937, 0.5590551181102362, 0.6220472440944882, 0.5748031496062992, 0.5826771653543307, 0.5826771653543307, 0.5826771653543307, 0.5590551181102362, 0.5590551181102362, 0.5748031496062992, 0.49606299212598426, 0.6141732283464567, 0.5826771653543307, 0.5905511811023622, 0.6535433070866141, 0.5905511811023622, 0.5748031496062992, 0.6377952755905512, 0.5590551181102362, 0.5511811023622047, 0.5748031496062992, 0.5669291338582677, 0.5118110236220472, 0.6062992125984252, 0.6220472440944882, 0.5748031496062992]\n",
      "Neuronal Network solver=sgd -> train scores: [0.7940594059405941, 0.7504950495049505, 0.7742574257425743, 0.7445544554455445, 0.7722772277227723, 0.7465346534653465, 0.7564356435643564, 0.7782178217821782, 0.7702970297029703, 0.7485148514851485, 0.7960396039603961, 0.7821782178217822, 0.7623762376237624, 0.7841584158415842, 0.7960396039603961, 0.7702970297029703, 0.7683168316831683, 0.807920792079208, 0.7643564356435644, 0.7980198019801981, 0.7504950495049505, 0.7960396039603961, 0.7564356435643564, 0.7861386138613862, 0.7603960396039604, 0.7881188118811882, 0.7762376237623763, 0.7603960396039604, 0.7782178217821782, 0.7702970297029703] test scores: [0.6850393700787402, 0.6929133858267716, 0.6377952755905512, 0.7086614173228346, 0.6929133858267716, 0.6535433070866141, 0.7401574803149606, 0.6535433070866141, 0.6062992125984252, 0.6692913385826772, 0.6614173228346457, 0.6299212598425197, 0.6299212598425197, 0.6377952755905512, 0.5905511811023622, 0.6377952755905512, 0.6929133858267716, 0.6456692913385826, 0.6535433070866141, 0.6535433070866141, 0.6850393700787402, 0.7086614173228346, 0.6771653543307087, 0.6377952755905512, 0.6141732283464567, 0.5984251968503937, 0.6299212598425197, 0.6141732283464567, 0.6614173228346457, 0.6377952755905512]\n",
      "Neuronal Network solver=adam -> train scores: [0.9564356435643564, 0.9405940594059405, 0.9564356435643564, 0.9386138613861386, 0.9564356435643564, 0.9603960396039604, 0.9445544554455445, 0.9524752475247524, 0.9564356435643564, 0.9465346534653465, 0.9524752475247524, 0.9524752475247524, 0.9504950495049505, 0.9544554455445544, 0.9524752475247524, 0.9425742574257425, 0.9425742574257425, 0.9465346534653465, 0.9544554455445544, 0.9524752475247524, 0.9504950495049505, 0.9465346534653465, 0.9564356435643564, 0.9584158415841584, 0.9544554455445544, 0.9564356435643564, 0.9544554455445544, 0.9425742574257425, 0.9564356435643564, 0.9564356435643564] test scores: [0.6062992125984252, 0.6692913385826772, 0.6299212598425197, 0.7007874015748031, 0.5905511811023622, 0.5590551181102362, 0.6456692913385826, 0.6220472440944882, 0.5984251968503937, 0.5826771653543307, 0.5905511811023622, 0.5984251968503937, 0.5590551181102362, 0.5826771653543307, 0.5039370078740157, 0.5905511811023622, 0.6220472440944882, 0.5669291338582677, 0.6535433070866141, 0.6062992125984252, 0.6141732283464567, 0.6614173228346457, 0.6062992125984252, 0.5196850393700787, 0.6141732283464567, 0.5196850393700787, 0.5905511811023622, 0.6062992125984252, 0.5669291338582677, 0.5826771653543307]\n"
     ]
    }
   ],
   "source": [
    "train_accuracy_LR_lbfgs = []\n",
    "test_accuracy_LR_lbfgs = []\n",
    "train_accuracy_LR_newton_cg = []\n",
    "test_accuracy_LR_newton_cg = []\n",
    "train_accuracy_LR_liblinear = []\n",
    "test_accuracy_LR_liblinear = []\n",
    "train_accuracy_LR_sag = []\n",
    "test_accuracy_LR_sag = []\n",
    "train_accuracy_LR_saga= []\n",
    "test_accuracy_LR_saga= []\n",
    "train_accuracy_SVM = []\n",
    "test_accuracy_SVM = []\n",
    "train_accuracy_RF = []\n",
    "test_accuracy_RF = []\n",
    "train_accuracy_NN_lbfgs = []\n",
    "test_accuracy_NN_lbfgs = []\n",
    "train_accuracy_NN_sgd = []\n",
    "test_accuracy_NN_sgd = []\n",
    "train_accuracy_NN_adam = []\n",
    "test_accuracy_NN_adam = []\n",
    "k_fold = KFold()\n",
    "for k, (train, test) in enumerate(k_fold.split(X, Y)):\n",
    "\n",
    "    LR_lbfgs = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr').fit(X[train], Y[train])\n",
    "    train_accuracy_LR_lbfgs.append(LR_lbfgs.score(X[train], Y[train]))\n",
    "    test_accuracy_LR_lbfgs.append(LR_lbfgs.score(X[test], Y[test]))\n",
    "    LR_newton_cg = LogisticRegression(random_state=0, solver='newton-cg', multi_class='ovr').fit(X[train], Y[train])\n",
    "    train_accuracy_LR_newton_cg.append(LR_newton_cg.score(X[train], Y[train]))\n",
    "    test_accuracy_LR_newton_cg.append(LR_newton_cg.score(X[test], Y[test]))\n",
    "    LR_liblinear = LogisticRegression(random_state=0, solver='liblinear', multi_class='ovr').fit(X[train], Y[train])\n",
    "    train_accuracy_LR_liblinear.append(LR_liblinear.score(X[train], Y[train]))\n",
    "    test_accuracy_LR_liblinear.append(LR_liblinear.score(X[test], Y[test]))\n",
    "    LR_sag = LogisticRegression(random_state=0, solver='sag', multi_class='ovr').fit(X[train], Y[train])\n",
    "    train_accuracy_LR_sag.append(LR_sag.score(X[train], Y[train]))\n",
    "    test_accuracy_LR_sag.append(LR_sag.score(X[test], Y[test]))\n",
    "    LR_saga = LogisticRegression(random_state=0, solver='saga', multi_class='ovr').fit(X[train], Y[train])\n",
    "    train_accuracy_LR_saga.append(LR_saga.score(X[train], Y[train]))\n",
    "    test_accuracy_LR_saga.append(LR_saga.score(X[test], Y[test]))\n",
    "\n",
    "    SVM = svm.SVC(kernel='linear', probability=True).fit(X[train], Y[train])\n",
    "    train_accuracy_SVM.append(SVM.score(X[train], Y[train]))\n",
    "    test_accuracy_SVM.append(SVM.score(X[test], Y[test]))\n",
    "\n",
    "    RF = RandomForestClassifier().fit(X[train], Y[train])\n",
    "    train_accuracy_RF.append(RF.score(X[train], Y[train]))\n",
    "    test_accuracy_RF.append(RF.score(X[test], Y[test]))\n",
    "\n",
    "    NN_lbfgs = MLPClassifier(solver='lbfgs', alpha=1e-5, random_state=1, max_iter=5000).fit(X[train], Y[train])\n",
    "    train_accuracy_NN_lbfgs.append(NN_lbfgs.score(X[train], Y[train]))\n",
    "    test_accuracy_NN_lbfgs.append(NN_lbfgs.score(X[test], Y[test]))\n",
    "    NN_sgd = MLPClassifier(solver='sgd', alpha=1e-5, random_state=1, max_iter=5000).fit(X[train], Y[train])\n",
    "    train_accuracy_NN_sgd.append(NN_sgd.score(X[train], Y[train]))\n",
    "    test_accuracy_NN_sgd.append(NN_sgd.score(X[test], Y[test]))\n",
    "    NN_adam = MLPClassifier(solver='adam', alpha=1e-5, random_state=1, max_iter=5000).fit(X[train], Y[train])\n",
    "    train_accuracy_NN_adam.append(NN_adam.score(X[train], Y[train]))\n",
    "    test_accuracy_NN_adam.append(NN_adam.score(X[test], Y[test]))\n",
    "\n",
    "print(f'Logistic Regression solver=lbfgs -> train scores: {train_accuracy_LR_lbfgs} test scores: {test_accuracy_LR_lbfgs}')\n",
    "print(f'Logistic Regression solver=newton-cg -> train scores: {train_accuracy_LR_newton_cg} '\n",
    "      f'test scores: {test_accuracy_LR_newton_cg}')\n",
    "print(f'Logistic Regression solver=liblinear -> train scores: {train_accuracy_LR_liblinear} '\n",
    "      f'test scores: {test_accuracy_LR_liblinear}')\n",
    "print(f'Logistic Regression solver=sag -> train scores: {train_accuracy_LR_sag} test scores: {test_accuracy_LR_sag}')\n",
    "print(f'Logistic Regression solver=saga -> train scores: {train_accuracy_LR_saga} test scores: {test_accuracy_LR_saga}')\n",
    "print(f'Support Vector Machines -> train scores: {train_accuracy_SVM} test scores: {test_accuracy_SVM}')\n",
    "print(f'Random Forest -> train scores: {train_accuracy_RF} test scores: {test_accuracy_RF}')\n",
    "print(f'Neuronal Network solver=lbfgs -> train scores: {train_accuracy_NN_lbfgs} test scores: {test_accuracy_NN_lbfgs}')\n",
    "print(f'Neuronal Network solver=sgd -> train scores: {train_accuracy_NN_sgd} test scores: {test_accuracy_NN_sgd}')\n",
    "print(f'Neuronal Network solver=adam -> train scores: {train_accuracy_NN_adam} test scores: {test_accuracy_NN_adam}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plotting the data\n",
    "With stored accuracy for training and test set of each model the mean precision and standard deviation are plotted."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ind: [0 1 2 3 4 5 6 7 8 9] Mean: (10,), sv (10,)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1120x960 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = ['LR-lbfgs', 'L-newton-cg', 'LR-liblinear','LR-sag', 'LR-saga', 'SVM', 'RF', 'NN-lbfgs', 'NN-sgd', 'NN-adam']\n",
    "ind = np.arange(10)\n",
    "train_means_list = [sum(train_accuracy_LR_lbfgs)/len(train_accuracy_LR_lbfgs),\n",
    "               sum(train_accuracy_LR_newton_cg)/len(train_accuracy_LR_newton_cg),\n",
    "               sum(train_accuracy_LR_liblinear)/len(train_accuracy_LR_liblinear),\n",
    "               sum(train_accuracy_LR_sag)/len(train_accuracy_LR_sag),\n",
    "               sum(train_accuracy_LR_saga)/len(train_accuracy_LR_saga),\n",
    "               sum(train_accuracy_SVM)/len(train_accuracy_SVM),\n",
    "               sum(train_accuracy_RF)/len(train_accuracy_RF),\n",
    "               sum(train_accuracy_NN_lbfgs)/len(train_accuracy_NN_lbfgs),\n",
    "               sum(train_accuracy_NN_sgd)/len(train_accuracy_NN_sgd),\n",
    "               sum(train_accuracy_NN_adam)/len(train_accuracy_NN_adam)\n",
    "]\n",
    "train_standar_deviation_list = [\n",
    "    statistics.stdev(train_accuracy_LR_lbfgs),\n",
    "    statistics.stdev(train_accuracy_LR_newton_cg),\n",
    "    statistics.stdev(train_accuracy_LR_liblinear),\n",
    "    statistics.stdev(train_accuracy_LR_sag),\n",
    "    statistics.stdev(train_accuracy_LR_saga),\n",
    "    statistics.stdev(train_accuracy_SVM),\n",
    "    statistics.stdev(train_accuracy_RF),\n",
    "    statistics.stdev(train_accuracy_NN_lbfgs),\n",
    "    statistics.stdev(train_accuracy_NN_sgd),\n",
    "    statistics.stdev(train_accuracy_NN_adam),\n",
    "]\n",
    "test_means_list = [sum(test_accuracy_LR_lbfgs)/len(test_accuracy_LR_lbfgs),\n",
    "               sum(test_accuracy_LR_newton_cg)/len(test_accuracy_LR_newton_cg),\n",
    "               sum(test_accuracy_LR_liblinear)/len(test_accuracy_LR_liblinear),\n",
    "               sum(test_accuracy_LR_sag)/len(test_accuracy_LR_sag),\n",
    "               sum(test_accuracy_LR_saga)/len(test_accuracy_LR_saga),\n",
    "               sum(test_accuracy_SVM)/len(test_accuracy_SVM),\n",
    "               sum(test_accuracy_RF)/len(test_accuracy_RF),\n",
    "               sum(test_accuracy_NN_lbfgs)/len(test_accuracy_NN_lbfgs),\n",
    "               sum(test_accuracy_NN_sgd)/len(test_accuracy_NN_sgd),\n",
    "               sum(test_accuracy_NN_adam)/len(test_accuracy_NN_adam)\n",
    "]\n",
    "test_standar_deviation_list = [\n",
    "    statistics.stdev(test_accuracy_LR_lbfgs),\n",
    "    statistics.stdev(test_accuracy_LR_newton_cg),\n",
    "    statistics.stdev(test_accuracy_LR_liblinear),\n",
    "    statistics.stdev(test_accuracy_LR_sag),\n",
    "    statistics.stdev(test_accuracy_LR_saga),\n",
    "    statistics.stdev(test_accuracy_SVM),\n",
    "    statistics.stdev(test_accuracy_RF),\n",
    "    statistics.stdev(test_accuracy_NN_lbfgs),\n",
    "    statistics.stdev(test_accuracy_NN_sgd),\n",
    "    statistics.stdev(test_accuracy_NN_adam),\n",
    "]\n",
    "train_means = np.array(train_means_list)\n",
    "train_standar_deviation = np.array(train_standar_deviation_list)\n",
    "test_means = np.array(test_means_list)\n",
    "test_standar_deviation = np.array(test_standar_deviation_list)\n",
    "print(f'ind: {ind} Mean: {test_means.shape}, sv {test_standar_deviation.shape}')\n",
    "width = 0.35\n",
    "plt.figure(figsize=(14, 12), dpi=80)\n",
    "plt.bar(ind, test_means, width=width, yerr=test_standar_deviation, label='Test Score')\n",
    "plt.bar(ind + width, train_means, width=width,yerr=train_standar_deviation, label='Train Score')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Scores of each model')\n",
    "plt.xticks(ind + width / 2, models)\n",
    "plt.legend()\n",
    "plt.savefig('models-study-figures/models-score.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Confusion Matrices\n",
    "For each model we generate a confusion matrix."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "plot_confusion_matrix(LR_lbfgs, x_test, y_test, display_labels=['Lose', 'Win'], cmap=plt.cm.Blues, normalize=None)\n",
    "\n",
    "plt.title('LR lbfgs solver confusion matrix')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}